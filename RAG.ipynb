{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bzn3PsAh2btg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers langchain langchain_community langchain-openai\n",
        "!pip install -U transformers accelerate\n",
        "!pip install tiktoken\n",
        "!pip install pinecone\n",
        "!pip install langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer, SimilarityFunction"
      ],
      "metadata": {
        "id": "i7gbWm-vOnSh"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "if hf_token is None:\n",
        "    raise ValueError(\"Hugging Face token not found. Please add it to Colab Secrets.\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "login(token=hf_token)\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"]=userdata.get('LG_SMITH')\n",
        "os.environ['LANGSMITH_PROJECT']=\"RAG\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=str(userdata.get('OPEN_AI')).strip()\n"
      ],
      "metadata": {
        "id": "A-SeBD9gIfFo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_embed = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", similarity_fn_name=SimilarityFunction.COSINE)"
      ],
      "metadata": {
        "id": "NrmmIPdYEqTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "VQIBN-TjX6KX"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Translation(Multi-Query)\n"
      ],
      "metadata": {
        "id": "wh0SGmPMDqlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
        "    \"\"\"Output parser for a list of lines.\"\"\"\n",
        "\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return list(filter(None, lines))\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate three\n",
        "    different versions of the given user question to retrieve relevant documents from a vector\n",
        "    database. By generating multiple perspectives on the user question, your goal is to help\n",
        "    the user overcome some of the limitations of the cosine-distance-based similarity search.\n",
        "    Provide these alternative questions + original question separated by newlines.Do not lable\n",
        "    alternative or original question with any text.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "\n",
        "MultyQueryGen = QUERY_PROMPT | llm | output_parser\n",
        "\n",
        "\n",
        "question = \"The students shall be informed, in an appropriate manner, of the exact examination regulations\"\n",
        "\n",
        "MultyQueryGen.invoke({\"question\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vKFt4GQEL0Z",
        "outputId": "6a8c4c23-daa1-466e-f1dd-dd3651addd60"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The students need to be notified of the specific examination rules in a suitable way.',\n",
              " 'Can the students be adequately informed about the precise examination regulations?',\n",
              " 'How can the students be informed effectively about the exact examination regulations?']"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Translation(Decomposition)"
      ],
      "metadata": {
        "id": "YuTJP8PfEBMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question}. Provide these sub-questions + original question separated by newlines.\\n\n",
        "Output (up to 3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0)\n",
        "\n",
        "\n",
        "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "\n",
        "question = \"If someone is accused of a crime, but they believe the evidence used against them was obtained unfairly, what can they do to challenge it?\"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
        "print(questions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHIq6BpoEIqw",
        "outputId": "f65eecb8-9024-499e-d9b3-69c7727a1f5d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1. What are the legal procedures for challenging evidence obtained unfairly in a criminal case?', '2. Are there specific laws or precedents that protect individuals from unfair evidence in criminal proceedings?', '3. How can a defense attorney argue for the exclusion of unfairly obtained evidence in court? ', '', 'If someone is accused of a crime, but they believe the evidence used against them was obtained unfairly, what can they do to challenge it?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FEdQIp5ODx43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever\n"
      ],
      "metadata": {
        "id": "FPVOJ4p2YSCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "import langchain_pinecone\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "index_name = \"rag-data\"\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pinecone_client = pinecone.Pinecone(api_key=userdata.get('PINE'))\n",
        "\n",
        "\n",
        "hf_model = model_embed\n",
        "\n",
        "\n",
        "class HuggingFaceEmbeddings(Embeddings):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self.model.encode(texts).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        \"\"\"Generate embedding for a single query.\"\"\"\n",
        "        return self.model.encode(text).tolist()\n",
        "\n",
        "hf_embeddings = HuggingFaceEmbeddings(hf_model)\n",
        "\n",
        "\n",
        "index_name = \"rag-data\"\n",
        "\n",
        "\n",
        "\n",
        "index = pinecone_client.Index(index_name)\n",
        "\n",
        "retriever= Pinecone(\n",
        "    embedding=hf_embeddings,\n",
        "    index=index,\n",
        "    text_key=\"Text\",\n",
        ").as_retriever()\n"
      ],
      "metadata": {
        "id": "simUZ4APYQTj",
        "collapsed": true
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiple_retrieve(question,sub_question_generator_chain):\n",
        "    \"\"\"Retreive on each sub-question/alternative-question\"\"\"\n",
        "\n",
        "    # Generate sub-questions/alternative questions\n",
        "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
        "\n",
        "    # Initialize a list to hold RAG chain results\n",
        "    retreive_results = []\n",
        "\n",
        "    for sub_question in sub_questions:\n",
        "\n",
        "        # Retrieve documents for each sub-question\n",
        "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
        "\n",
        "\n",
        "        retreive_results.append((sub_question, retrieved_docs))\n",
        "\n",
        "    return retreive_results"
      ],
      "metadata": {
        "id": "unEw8kg7cjju"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strucuted Grade of Retrieval"
      ],
      "metadata": {
        "id": "sePnMNLFa1Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain import hub\n",
        "\n",
        "\n",
        "class GradeDocuments(BaseModel):\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Are documents relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "grade_prompt = hub.pull(\"efriis/self-rag-retrieval-grader\")\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "\n",
        "def GradeDocs(retreive_results):\n",
        "    \"\"\"Grade the retrieved documents and return formatted text\"\"\"\n",
        "\n",
        "    retreive_results_useful = []\n",
        "\n",
        "    formatted_result = \"\"\n",
        "\n",
        "    for sub_question, retrieved_docs in retreive_results:\n",
        "        print(f\"Processing Sub Query: {sub_question}\")\n",
        "        for doc in retrieved_docs:\n",
        "            grade = retrieval_grader.invoke({\"question\": sub_question, \"document\": doc.page_content})\n",
        "            if grade.binary_score == \"yes\":\n",
        "                retreive_results_useful.append((sub_question, retrieved_docs))\n",
        "                formatted_result += f\"Sub Query: {sub_question}\\n\"\n",
        "                formatted_result += \"Retrieved Documents:\\n\"\n",
        "                for doc_for_formatting in retrieved_docs:\n",
        "                  formatted_result += f\"- {doc_for_formatting.page_content}\\n\"\n",
        "                formatted_result += \"\\n\"\n",
        "\n",
        "    return formatted_result\n"
      ],
      "metadata": {
        "id": "AxIjnALGhsPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f71a18-5995-4189-98a6-2dfbacd222a2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py:1354: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the retrieval grader\n",
        "question = \"damage caused by a nuclear event?\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[0].page_content\n",
        "print(doc_txt)\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRl2ANGbBv9u",
        "outputId": "c335961b-73da-479a-d686-e4577d29802e"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "References:\n",
            "Text:This Federal Act shall not apply to damage caused by a nuclear event covered by an international convention ratified by EFTA states and EC Member States.\n",
            "Provision for Coverage\n",
            "binary_score='yes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta-Data Augmentation(does not show expected effectiveness)\n"
      ],
      "metadata": {
        "id": "T7-JvE2ioSX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# def format_references_context(documents):\n",
        "\n",
        "#     formatted_list = []\n",
        "\n",
        "#     for doc in documents:\n",
        "#         act_name = doc.metadata.get(\"ActName\", \"Unknown Act\")\n",
        "#         page_content = doc.page_content\n",
        "\n",
        "#         formatted_text = f\"Act Name: {act_name}\\nContent: {page_content}\"\n",
        "#         formatted_list.append(formatted_text)\n",
        "\n",
        "#     return formatted_list\n",
        "\n",
        "# template = \"\"\"\n",
        "# You are a helpful assistant that generates a single, comprehensive search query based on the references mentioned in the text.\n",
        "# Your goal is to create a unified and contextually relevant search query that captures how all the references are mentioned and described in the text.\n",
        "\n",
        "# Context:\n",
        "# {document}\n",
        "\n",
        "# Instructions:\n",
        "# - Identify and extract all references mentioned in the text.\n",
        "# - Analyze the context around each reference to understand its description, purpose, or significance.\n",
        "# - Combine the extracted information about all references into one detailed and semantically meaningful search query.\n",
        "# - Do not return the original text\n",
        "\n",
        "# Output:\n",
        "# Unified Search Query: <Generated query that includes all references and their contextual descriptions>\n",
        "# \"\"\"\n",
        "\n",
        "# prompt_refs = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "\n",
        "# # Use prompt_refs instead of prompt_decomposition\n",
        "# retreive_references = ( prompt_refs | llm | StrOutputParser()|retriever.get_relevant_documents|format_references_context)\n",
        "\n",
        "\n",
        "# document = \"\"\"References:Para 1 Para 4 Para 2 Para 3\n",
        "# Text:. The Federal Minister for European and International Affairs shall inform the National and Federal Councils each half-year about the projects of the European Union, as announced by the respective competent Federal Minister, on which the Council is expected to embark on deliberations within the following six months, if these projects\n",
        "# \t1.\tresult in a modification of the contractual bases of the European Union or\n",
        "# \t2.\tare subject to a special right of participation of the National and Federal Councils under Art. 23i and Art. 23j Federal Constitutional Law or\n",
        "\n",
        "# \t3.\tare subject to a special information duty under Art. 23e para 2 Federal Constitutional Law or\n",
        "\n",
        "# \t4.\tare decisions to extend the competences under Art. 82 para 2 lit. d TFEU, Art. 83 para 1 sub-para 3 TFEU and Art. 86 para 4 TFEU or\n",
        "# \t5.\tare aimed at establishing enhanced co¬operation under Art. 20 TEU or\n",
        "# \t6.\tconcern negotiating mandates for the Commission with regard to international treaties or\n",
        "# \t7.\tnegotiating guidelines for the Commission within the framework of the common commercial policy or\n",
        "# \t8.\tare of special importance for the Republic of Austria.\n",
        "# Written Information\"\"\"\n",
        "# # questions = generate_query_for_reference.invoke({\"document\": document})\n",
        "# # print(questions)\n",
        "\n",
        "\n",
        "# def MetaDataAugmentation(retreive_results):\n",
        "#     \"\"\"Second order retreive based on metadata\"\"\"\n",
        "#     retreive_results_usefull = []\n",
        "#     for document in retreive_results:\n",
        "#         retreive_meta_data = retreive_references.invoke({\"document\": document})\n",
        "#         retreive_meta_data = []\n",
        "\n",
        "#         result_text = \"possible references context:\\n\"\n",
        "#         for i in range(len(retreive_meta_data)):\n",
        "#           if i == 0:\n",
        "#             continue\n",
        "#           else:\n",
        "#             if retrieval_grader.invoke({\"question\": sub_question, \"documents\": retrieved_docs})\n",
        "#             result_text += f\"*{retreive_meta_data[i]}*\\n\"\n",
        "\n",
        "\n",
        "#     return retreive_results\n",
        "# print(retreive_references.invoke(document))"
      ],
      "metadata": {
        "id": "_FEgqbkLoQ7F"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "vW_nJbIDn2At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"You are an assistant tasked with summarizing legal information and explaining it in simple and clear terms for people without specialized legal education.\n",
        "\n",
        "Task:\n",
        "1. Provide a brief summary of the legal content.\n",
        "2. Use clear and accessible language, avoiding legal jargon wherever possible.\n",
        "3. Include practical examples or analogies if necessary to help the audience understand.\n",
        "4. Focus on the key points and their implications for an ordinary person.\n",
        "\n",
        "Example:\n",
        "\n",
        "1. Query: What does § 12 cover?\n",
        "   Legal Context: § 12 discusses taxation policies. It specifies that these policies apply to auctions and works of art. The section outlines the requirements for reporting and calculating taxes during these transactions.\n",
        "   Output:\n",
        "   - Summary: § 12 focuses on taxation policies and explains how they apply to auctions and works of art.\n",
        "   - Explanation: This section of the law describes how taxes should be calculated and reported for auctions and the sale of works of art. For example, if someone sells a painting at an auction, the seller needs to ensure that the taxes are calculated and reported correctly. This ensures transparency and helps prevent disputes about taxation.\n",
        "\n",
        "\n",
        "\n",
        "Now complete the task for the following input:\n",
        "\n",
        "Query: {query}\n",
        "Legal Context: {context}\n",
        "\n",
        "Output:\n",
        "- Summary: <Provide a brief and clear summary of the legal content.>\n",
        "- Explanation: <Explain the content in simple terms, addressing the query and making it relatable for a non-expert audience.> \"\"\")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "query = \"damage caused by a nuclear event?\"    #Query\n",
        "retreived_docs = multiple_retrieve(query,MultyQueryGen) #Query Translation\n",
        "useful_docs = GradeDocs(retreived_docs)     #Document Grading\n",
        "\n",
        "\n",
        "generation = rag_chain.invoke({\"context\": useful_docs, \"query\": query})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i67bVYuhnowp",
        "outputId": "a3ad38fb-f606-4c98-fc35-60e8994cdadb"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Sub Query: What are the consequences of a nuclear incident?\n",
            "Processing Sub Query: Effects of a nuclear event?\n",
            "Processing Sub Query: What kind of harm can be attributed to a nuclear disaster?\n",
            "- Summary: The legal provision states that damage caused by a nuclear event covered by an international convention ratified by EFTA states and EC Member States is not covered by this Federal Act.\n",
            "- Explanation: This means that if a nuclear event occurs and causes damage, the rules and regulations outlined in this Federal Act do not apply if the event is covered by an international convention ratified by certain countries. For example, if a nuclear accident happens and is governed by an international agreement, the legal framework in this Federal Act may not be relevant in determining liability or compensation for the damage caused.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hallucination Detection"
      ],
      "metadata": {
        "id": "Oapm9vXtT74C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "hallucination_prompt = PromptTemplate(\n",
        "    input_variables=[\"documents\", \"response\"],\n",
        "    template=\"\"\"You are an assistant that evaluates whether a given response is supported by the provided documents.\n",
        "\n",
        "Task:\n",
        "- Carefully analyze the response and compare it with the provided documents.\n",
        "- Determine if all the claims made in the response are explicitly supported by the content of the documents.\n",
        "- If there is any part of the response that is not directly supported by the documents, the answer should be \"no\".\n",
        "- If every claim in the response is backed by the documents, the answer should be \"yes\".\n",
        "\n",
        "Context:\n",
        "Documents: {documents}\n",
        "\n",
        "Response to Evaluate: {response}\n",
        "\n",
        "Output:\n",
        "Answer (yes/no): <Answer>\"\"\"\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "print(generation)\n",
        "print(hallucination_grader.invoke({\"documents\": useful_docs, \"response\": generation}))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T_rEzHtg351R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff2596c-457d-4a00-8753-94298fc2f1ae"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Summary: The legal provision states that damage caused by a nuclear event covered by an international convention ratified by EFTA states and EC Member States is not covered by this Federal Act.\n",
            "- Explanation: This means that if a nuclear event occurs and causes damage, the rules and regulations outlined in this Federal Act do not apply if the event is covered by an international convention ratified by certain countries. For example, if a nuclear accident happens and is governed by an international agreement, the legal framework in this Federal Act may not be relevant in determining liability or compensation for the damage caused.\n",
            "binary_score='yes'\n"
          ]
        }
      ]
    }
  ]
}