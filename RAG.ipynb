{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Bzn3PsAh2btg"
      },
      "outputs": [],
      "source": [
        "!pip install transformers langchain langchain_community langchain-openai\n",
        "!pip install -U transformers accelerate\n",
        "!pip install tiktoken\n",
        "!pip install pinecone\n",
        "!pip install langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "i7gbWm-vOnSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "if hf_token is None:\n",
        "    raise ValueError(\"Hugging Face token not found. Please add it to Colab Secrets.\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "login(token=hf_token)\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"]=userdata.get('LG_SMITH')\n",
        "os.environ['LANGSMITH_PROJECT']=\"RAG\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=str(userdata.get('OPEN_AI')).strip()\n"
      ],
      "metadata": {
        "id": "A-SeBD9gIfFo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_embed = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", similarity_fn_name=SimilarityFunction.COSINE)"
      ],
      "metadata": {
        "id": "NrmmIPdYEqTG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "VQIBN-TjX6KX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Translation(Multi-Query)\n"
      ],
      "metadata": {
        "id": "wh0SGmPMDqlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considering the project's specifics and target audience, we assume that potential users might lack legal education. Their questions may be poorly structured or ambiguous, making accurate semantic search challenging. To address this, we generate multiple refined sub-queries based on the user's initial query. This approach ensures better semantic coverage, resolves ambiguities, and improves the retrieval of relevant legal information.\n",
        "\n"
      ],
      "metadata": {
        "id": "Za2KhBhrNpQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
        "    \"\"\"Output parser for a list of lines.\"\"\"\n",
        "\n",
        "    def parse(self, text: str) -> List[str]:\n",
        "        lines = text.strip().split(\"\\n\")\n",
        "        return list(filter(None, lines))\n",
        "\n",
        "\n",
        "output_parser = LineListOutputParser()\n",
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate three\n",
        "    different versions of the given user question to retrieve relevant documents from a vector\n",
        "    database. By generating multiple perspectives on the user question, your goal is to help\n",
        "    the user overcome some of the limitations of the cosine-distance-based similarity search.\n",
        "    Provide these alternative questions + original question separated by newlines.Do not lable\n",
        "    alternative or original question with any text.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "\n",
        "\n",
        "MultyQueryGen = QUERY_PROMPT | llm | output_parser\n",
        "\n",
        "\n",
        "question = \"The students shall be informed, in an appropriate manner, of the exact examination regulations\"\n",
        "\n",
        "MultyQueryGen.invoke({\"question\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vKFt4GQEL0Z",
        "outputId": "f29a37d7-e66e-4d19-e33f-15cce1bfb0c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The students need to be notified of the specific examination rules in a suitable way.',\n",
              " 'How can the students be properly informed about the exact examination regulations?',\n",
              " 'What is the appropriate method to inform the students about the exact examination regulations?']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Translation(Decomposition)"
      ],
      "metadata": {
        "id": "YuTJP8PfEBMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to the scenario addressed in the previous method, this approach considers cases where the user's query is a complex, hierarchically structured question. In such instances, it makes sense to decompose the query into its constituent parts to ensure broader contextual coverage and retrieve the most relevant information for each sub-question."
      ],
      "metadata": {
        "id": "SDVvAEvWPG9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question}. Provide these sub-questions + original question separated by newlines.\\n\n",
        "Output (up to 3 queries):\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",temperature=0)\n",
        "\n",
        "\n",
        "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
        "\n",
        "\n",
        "question = \"If someone is accused of a crime, but they believe the evidence used against them was obtained unfairly, what can they do to challenge it?\"\n",
        "questions = generate_queries_decomposition.invoke({\"question\":question})\n",
        "print(questions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHIq6BpoEIqw",
        "outputId": "d98a31f4-51a8-41bc-e2e0-0d599d3a4a97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['- What are the legal procedures for challenging evidence obtained unfairly in a criminal case?', '- Are there specific laws or regulations that protect individuals from unfair evidence in court?', '- How can a defense attorney argue against the admissibility of unfairly obtained evidence in a trial? ', '', 'If someone is accused of a crime, but they believe the evidence used against them was obtained unfairly, what can they do to challenge it?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever\n"
      ],
      "metadata": {
        "id": "FPVOJ4p2YSCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the retriever, we utilize a vector database search, using cosine distance as the metric to measure similarity between the query and stored document embeddings."
      ],
      "metadata": {
        "id": "W8tqFiq-Pq_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pinecone\n",
        "import langchain_pinecone\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings.base import Embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pinecone_client = pinecone.Pinecone(api_key=userdata.get('PINE'))\n",
        "\n",
        "\n",
        "hf_model = model_embed\n",
        "\n",
        "\n",
        "class HuggingFaceEmbeddings(Embeddings):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
        "        return self.model.encode(texts).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        \"\"\"Generate embedding for a single query.\"\"\"\n",
        "        return self.model.encode(text).tolist()\n",
        "\n",
        "hf_embeddings = HuggingFaceEmbeddings(hf_model)\n",
        "\n",
        "\n",
        "index_name = \"rag-data-paragraphs\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "index = pinecone_client.Index(index_name)\n",
        "\n",
        "class Retriever:\n",
        "    def __init__(self, index, embedder):\n",
        "        self.index = index\n",
        "        self.embedder = embedder\n",
        "\n",
        "    def get_relevant_documents(self, query):\n",
        "        query_vector = self.embedder.embed_query(query)\n",
        "\n",
        "        response = self.index.query(\n",
        "            vector=query_vector,\n",
        "            top_k=5,\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        results = []\n",
        "        for match in response[\"matches\"]:\n",
        "            cosine_similarity = match['score']\n",
        "            cosine_distance = 1 - cosine_similarity\n",
        "            results.append({\n",
        "                \"text\": match['metadata']['Text'],\n",
        "                \"metadata\": match['metadata'],\n",
        "                \"cosine_similarity\": cosine_similarity,\n",
        "                \"cosine_distance\": cosine_distance\n",
        "            })\n",
        "        return results\n",
        "\n",
        "retriever = Retriever(index, hf_embeddings)"
      ],
      "metadata": {
        "id": "simUZ4APYQTj",
        "collapsed": true
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiple_retrieve(question,sub_question_generator_chain):\n",
        "    \"\"\"Retreive on each sub-question/alternative-question\"\"\"\n",
        "\n",
        "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
        "\n",
        "    retreive_results = []\n",
        "\n",
        "    for sub_question in sub_questions:\n",
        "\n",
        "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
        "        print(f'Sub-Question:{sub_question}, Retrieved Documents: {len(retrieved_docs)}')\n",
        "        for doc in retrieved_docs:\n",
        "          if doc not in retreive_results:\n",
        "              retreive_results.append(doc)\n",
        "    print(\"Retrieved Unique Documents: \",len(retreive_results))\n",
        "    return retreive_results"
      ],
      "metadata": {
        "id": "unEw8kg7cjju"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ranking"
      ],
      "metadata": {
        "id": "myAHUPcXwQ_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_docs(documents):\n",
        "    \"\"\"Rank the documents based on their cosine-similarity to the query.\"\"\"\n",
        "    docs = [doc for doc in documents if doc['cosine_similarity'] >= 0.75]\n",
        "    print(\"Ranked Documents: \",len(docs))\n",
        "    return sorted(documents, key=lambda x: x['cosine_similarity'], reverse=True)"
      ],
      "metadata": {
        "id": "sn5iMEpmwQJo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strucuted Grade of Retrieval\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sePnMNLFa1Y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For retrieving and grading a large number of documents"
      ],
      "metadata": {
        "id": "LNzmsX7LQOAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain import hub\n",
        "\n",
        "\n",
        "class GradeDocuments(BaseModel):\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Are documents relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "grade_prompt = hub.pull(\"efriis/self-rag-retrieval-grader\")\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "\n",
        "def GradeDocs(query, retreive_results):\n",
        "    \"\"\"Grade the retrieved documents and return formatted text\"\"\"\n",
        "\n",
        "    retreive_results_useful = []\n",
        "    for doc in retreive_results:\n",
        "        grade = retrieval_grader.invoke({\"question\": query, \"document\": doc['text']})\n",
        "        if grade.binary_score == \"yes\":\n",
        "            retreive_results_useful.append(doc)\n",
        "\n",
        "\n",
        "    return retreive_results_useful\n"
      ],
      "metadata": {
        "id": "AxIjnALGhsPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The re-ranking process utilizes a large language model (LLM) as an AI agent to refine the initial retrieval results. The AI agent evaluates the contextual relevance of each document in relation to the query, leveraging its advanced understanding of semantics and linguistic patterns."
      ],
      "metadata": {
        "id": "PQzUlG_nR2YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import RegexParser\n",
        "\n",
        "class RelevanceScore(BaseModel):\n",
        "    relevance_score: float = Field(\n",
        "        ge=0.0, le=1.0,\n",
        "        description=\"Relevance score of the document to the question. 0 means completely irrelevant, 1 means completely relevant.\"\n",
        "    )\n",
        "\n",
        "\n",
        "rerank_prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are a professional document relevance grader. For each document provided, assign a relevance score between 0 and 1.\n",
        "The score should reflect how well the document answers the provided question:\n",
        "- 0 means completely irrelevant.\n",
        "- 1 means highly relevant.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Document: {document}\n",
        "\n",
        "Relevance Score (0-1):\n",
        "\"\"\")\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
        "structured_llm_reranker = llm.with_structured_output(RelevanceScore, method=\"function_calling\")\n",
        "\n",
        "output_parser = RegexParser(\n",
        "    regex=r\"Relevance Score \\(0-1\\): (.*)\",\n",
        "    output_keys=[\"relevance_score\"],\n",
        "    default_output_key=\"relevance_score\"\n",
        ")\n",
        "\n",
        "retrieval_ranker = rerank_prompt | structured_llm_reranker\n",
        "\n",
        "\n",
        "def rerank_docs(documents, query):\n",
        "    \"\"\"Rank the documents based on their relevance to the query.\"\"\"\n",
        "\n",
        "    ranked_docs = []\n",
        "    for doc in documents:\n",
        "        grade = retrieval_ranker.invoke({\"question\": query, \"document\": doc['text']})\n",
        "        doc['relevance_score'] = grade.relevance_score\n",
        "        ranked_docs.append(doc)\n",
        "    return sorted(ranked_docs, key=lambda x: x['relevance_score'], reverse=True)"
      ],
      "metadata": {
        "id": "J8h0BC6fv5AA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "vW_nJbIDn2At"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generation process leverages techniques such as prompt engineering and few-shot learning (providing a few examples in the prompt) to guide the model in producing accurate and contextually relevant outputs."
      ],
      "metadata": {
        "id": "VM4bHCLXSQnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"You are a professional legal assistant tasked with providing clear, accurate, and detailed answers to legal questions. Your goal is to directly address the query by including specific details (such as numbers, dates, or examples) and providing a concise explanation to help non-experts understand the context.\n",
        "\n",
        "### Task:\n",
        "1. **Direct Answer**: Start by providing a precise and concise answer to the question, including all relevant details, such as numerical values, dates, or other specifics from the legal context.\n",
        "2. **Explanation**: Follow the answer with a clear explanation in simple terms, avoiding legal jargon. Use examples or analogies if necessary to make the information relatable and easier to understand.\n",
        "\n",
        "### Important Notes:\n",
        "- Always prioritize specific details if they are present in the provided context.\n",
        "- Avoid overly general or vague responses.\n",
        "- Ensure the explanation directly relates to the user's query and provides actionable or practical insights.\n",
        "\n",
        "Example:\n",
        "\n",
        "1. Query: Can I break my lease early if I lose my job?\n",
        "   Legal Context: Tenancy agreements often include clauses about early termination. In many jurisdictions, tenants can break a lease early if they provide proper notice and a valid reason, such as job loss. However, penalties or fees may apply unless specified otherwise in the contract.\n",
        "   Output:\n",
        "   - Answer: You may be able to break your lease early if you lose your job, but fees or penalties might apply depending on your contract.\n",
        "   - Explanation: Check your lease agreement for an early termination clause. Many agreements allow breaking the lease if you provide written notice, but you might have to pay a penalty or forfeit your deposit. For example, if you have six months left on your lease, your landlord might require payment for one or two months as a penalty.\n",
        "\n",
        "2.  Query: What should I do if my employer doesn’t pay me on time?\n",
        "   Legal Context: Labor laws typically require employers to pay employees on the agreed-upon schedule. Late payments may be a violation of these laws, and employees can file a complaint with the local labor board or seek legal assistance to recover unpaid wages.\n",
        "   Output:\n",
        "   - Answer: If your employer doesn’t pay you on time, you can file a complaint with the labor board or pursue legal action to recover unpaid wages.\n",
        "   - Explanation: Employers are legally required to pay employees on time. If your payment is late, start by contacting your employer to resolve the issue informally. If that doesn’t work, you can file a formal complaint with the labor board. For instance, if your paycheck is delayed by more than a week, you could report this as a violation of labor laws to protect your rights.\n",
        "\n",
        "Now complete the task for the following input:\n",
        "\n",
        "Query: {query}\n",
        "Legal Context: {context}\n",
        "\n",
        "Output:\n",
        "- Answer: <Provide a concise and direct answer to the query.>\n",
        "- Explanation: <If needed, explain the content in simple terms, addressing the query and making it relatable for a non-expert audience.> \"\"\")\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "query = \"What are the working hour limits for employees residing in their employer's household, and how do they differ based on age and year of enforcement?\"    #Query\n",
        "print(f'Query: {query} \\n')\n",
        "retreived_docs = multiple_retrieve(query,generate_queries_decomposition) #Query Translation\n",
        "ranked_docs = rank_docs(retreived_docs) #Ranking\n",
        "reranked_docs = rerank_docs(ranked_docs,query) #Re-ranking\n",
        "context = ''\n",
        "# ensures that the length of content fits in model's content window\n",
        "for doc in reranked_docs:\n",
        "  if len(word_tokenize(context)) + len(word_tokenize(doc['text'])) < 3000:\n",
        "    context = context + doc['metadata']['ActName'] + \" \" + doc['metadata']['Section'] + \" \" + doc['metadata']['Paragraph'] + doc['text'] + \"\\n\\n\"\n",
        "  else:\n",
        "    break\n",
        "print(\"Length of context:\", len(word_tokenize(context)))\n",
        "generation = rag_chain.invoke({\"context\": context, \"query\": query})\n",
        "print(generation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i67bVYuhnowp",
        "outputId": "81d344ce-53a2-47ff-a3bb-22368df7cecb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What are the working hour limits for employees residing in their employer's household, and how do they differ based on age and year of enforcement? \n",
            "\n",
            "Sub-Question:- What are the current working hour limits for employees residing in their employer's household?, Retrieved Documents: 5\n",
            "Sub-Question:- How do working hour limits vary based on the age of employees residing in their employer's household?, Retrieved Documents: 5\n",
            "Sub-Question:- How have working hour limits for employees residing in their employer's household changed over the years in terms of enforcement?, Retrieved Documents: 5\n",
            "Retrieved Unique Documents:  15\n",
            "Ranked Documents:  3\n",
            "Length of context: 2072\n",
            "- Answer: For employees residing in their employer's household, the working hours within two calendar weeks should not exceed 106 hours for those under 18 years of age and 116 hours for those 18 and older, as of 5 January 1970. These limits were reduced to 104 hours and 114 hours respectively from 3 January 1972, and further reduced to 100 hours and 110 hours from 6 January 1975. \n",
            "\n",
            "- Explanation: This means that if you are an employee living in your employer's household, the maximum amount of time you can be asked to work within a two-week period depends on your age and the year. For example, if you were under 18 and working in 1970, you could be asked to work up to 106 hours in two weeks. But if you were the same age and working in 1975, you could only be asked to work up to 100 hours in two weeks. The same reductions apply if you are 18 or older, but the maximum hours are slightly higher. These limits include the time you need to be available for work, not just the time you are actively working.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hallucination Detection"
      ],
      "metadata": {
        "id": "Oapm9vXtT74C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "hallucination_prompt = PromptTemplate(\n",
        "    input_variables=[\"documents\", \"response\"],\n",
        "    template=\"\"\"You are an assistant that evaluates whether a given response is supported by the provided documents.\n",
        "\n",
        "Task:\n",
        "- Carefully analyze the response and compare it with the provided documents.\n",
        "- Determine if all the claims made in the response are explicitly supported by the content of the documents.\n",
        "- If there is any part of the response that is not directly supported by the documents, the answer should be \"no\".\n",
        "- If every claim in the response is backed by the documents, the answer should be \"yes\".\n",
        "\n",
        "Context:\n",
        "Documents: {documents}\n",
        "\n",
        "Response to Evaluate: {response}\n",
        "\n",
        "Output:\n",
        "Answer (yes/no): <Answer>\"\"\"\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "print(generation)\n",
        "print(hallucination_grader.invoke({\"documents\": useful_docs, \"response\": generation}))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T_rEzHtg351R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af33d1d2-aa17-4512-c188-f68e2dc65520"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Answer: For employees residing in their employer's household, the working hours within two calendar weeks should not exceed 106 hours for those under 18 years of age and 116 hours for those 18 and older, as of 5 January 1970. These limits were reduced to 104 hours and 114 hours respectively from 3 January 1972, and further reduced to 100 hours and 110 hours from 6 January 1975. \n",
            "\n",
            "- Explanation: This means that if you are an employee living in your employer's household, the maximum amount of time you can be asked to work within a two-week period depends on your age and the year. For example, if you were under 18 and working in 1970, you could be asked to work up to 106 hours in two weeks. But if you were the same age and working in 1975, you could only be asked to work up to 100 hours in two weeks. The same reductions apply if you are 18 or older, but the maximum hours are slightly higher. These limits include the time you need to be available for work, not just the time you are actively working.\n",
            "binary_score='yes'\n"
          ]
        }
      ]
    }
  ]
}